{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Neural Style Transfer.ipynb","provenance":[{"file_id":"1fJ7nBQL7uhFBShetEDos-of7b9N71mvR","timestamp":1577744365402},{"file_id":"1gDuyL37Wm7AA-icOUHs79TDtvYS2rMMC","timestamp":1577715267591}],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QblRyjA3xJNb"},"source":["### Tensorflow Version Check"]},{"cell_type":"code","metadata":{"id":"N5qLyvPWw2gV","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","print(tf.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eqxUicSPUOP6","colab_type":"text"},"source":["### Import and configure modules"]},{"cell_type":"code","metadata":{"id":"sc1OLbOWhPCO","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","mpl.rcParams['figure.figsize'] = (10,10)\n","mpl.rcParams['axes.grid'] = False\n","\n","import numpy as np\n","from PIL import Image\n","import time\n","import functools\n","\n","import os\n","from google.colab import files"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RYEjlrYk3s6w","colab_type":"code","colab":{}},"source":["from tensorflow.python.keras.preprocessing import image as kp_image\n","from tensorflow.python.keras import models \n","from tensorflow.python.keras import losses\n","from tensorflow.python.keras import layers\n","from tensorflow.python.keras import backend as K"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dcZ2NBSrxI4e"},"source":["### Google Drive Mount"]},{"cell_type":"code","metadata":{"id":"48MQ1nInxF_q","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yw50huyOnGbt","colab_type":"code","colab":{}},"source":["# style_path = '/content/drive/Shared drives/Deep Six Design/Deep Six Design/Projects/Neural Art Process/Iterations/Contact/style.png'\n","style_path = '/content/drive/Shared drives/Deep Six Design/Deep Six Design/Projects/Neural Art Process/alexgrey_images/alexgrey_images/00000000.jpg'\n","content_path = '/content/drive/Shared drives/Deep Six Design/Deep Six Design/Projects/Neural Art Process/Iterations/Contact/content.png'\n","output_path = '/content/drive/Shared drives/Deep Six Design/Deep Six Design/Projects/Neural Art Process/Iterations/Contact/output.png'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xE4Yt8nArTeR","colab_type":"text"},"source":["## Visualize the input"]},{"cell_type":"code","metadata":{"id":"3TLljcwv5qZs","colab_type":"code","colab":{}},"source":["def load_img(path_to_img):\n","  max_dim = 2048\n","  img = Image.open(path_to_img)\n","  long = max(img.size)\n","  scale = max_dim/long\n","  img = img.resize((round(img.size[0]*scale), round(img.size[1]*scale)), Image.ANTIALIAS)\n","  \n","  img = kp_image.img_to_array(img)\n","  \n","  # Broadcast the image array such that it has a batch dimension \n","  img = np.expand_dims(img, axis=0)\n","  return img"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vupl0CI18aAG","colab_type":"code","colab":{}},"source":["def imshow(img, title=None):\n","  # Remove the batch dimension\n","  out = np.squeeze(img, axis=0)\n","  # Normalize for display \n","  out = out.astype('uint8')\n","  plt.imshow(out)\n","  if title is not None:\n","    plt.title(title)\n","  plt.imshow(out)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hGwmTwJNmv2a","colab_type":"code","colab":{}},"source":["# Load preprocess images according to that VGG train process. each channel is normalized by mean = [103.939, 116.779, 123.68] and with channels BGR.\n","def load_and_process_img(path_to_img):\n","  img = load_img(path_to_img)\n","  img = tf.keras.applications.vgg19.preprocess_input(img)\n","  return img"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mjzlKRQRs_y2","colab_type":"code","colab":{}},"source":["# Inverse the preprocess step to view the ouptuts of the optimization. clip values to 0-255 from infinity\n","def deprocess_img(processed_img):\n","  x = processed_img.copy()\n","  if len(x.shape) == 4:\n","    x = np.squeeze(x, 0)\n","  assert len(x.shape) == 3, (\"Input to deprocess image must be an image of \"\n","                             \"dimension [1, height, width, channel] or [height, width, channel]\")\n","  if len(x.shape) != 3:\n","    raise ValueError(\"Invalid input to deprocessing image\")\n","  \n","  # perform the inverse of the preprocessiing step\n","  x[:, :, 0] += 103.939\n","  x[:, :, 1] += 116.779\n","  x[:, :, 2] += 123.68\n","  x = x[:, :, ::-1]\n","\n","  x = np.clip(x, 0, 255).astype('uint8')\n","  return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N4-8eUp_Kc-j","colab_type":"code","colab":{}},"source":["# pull feature maps from these content layers\n","content_layers = ['block5_conv2'] \n","\n","# interested in these style layers\n","style_layers = ['block1_conv1',\n","                'block2_conv1',\n","                'block3_conv1', \n","                'block4_conv1', \n","                'block5_conv1'\n","               ]\n","\n","num_content_layers = len(content_layers)\n","num_style_layers = len(style_layers)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nfec6MuMAbPx","colab_type":"code","colab":{}},"source":["# Access intermediate layers of style and features by getting corresponding outputs with Keras\n","# define the model via functional api like this... model = Model(inputs, outputs)\n","def get_model():\n","  # Load pretrained VGG, trained on imagenet data\n","  vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n","  vgg.trainable = False\n","  # Get output layers corresponding to style and content layers \n","  style_outputs = [vgg.get_layer(name).output for name in style_layers]\n","  content_outputs = [vgg.get_layer(name).output for name in content_layers]\n","  model_outputs = style_outputs + content_outputs\n","  # Build model \n","  return models.Model(vgg.input, model_outputs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d2mf7JwRMkCd","colab_type":"code","colab":{}},"source":["# add content losses to each layer\n","def get_content_loss(base_content, target):\n","  return tf.reduce_mean(tf.square(base_content - target))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N7MOqwKLLke8","colab_type":"code","colab":{}},"source":["# implement style lose as a distance metric\n","def gram_matrix(input_tensor):\n","  # Make image channels first \n","  channels = int(input_tensor.shape[-1])\n","  a = tf.reshape(input_tensor, [-1, channels])\n","  n = tf.shape(a)[0]\n","  gram = tf.matmul(a, a, transpose_a=True)\n","  return gram / tf.cast(n, tf.float32)\n","\n","def get_style_loss(base_style, gram_target):\n","  \"\"\"Expects two images of dimension h, w, c\"\"\"\n","  # height, width, num filters of each layer\n","  # Scale the loss at a given layer by the size of the feature map and the number of filters\n","  height, width, channels = base_style.get_shape().as_list()\n","  gram_style = gram_matrix(base_style)\n","  \n","  return tf.reduce_mean(tf.square(gram_style - gram_target))# / (4. * (channels ** 2) * (width * height) ** 2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O-lj5LxgtmnI","colab_type":"code","colab":{}},"source":["# load content and style, feed them through the network and output the representations\n","def get_feature_representations(model, content_path, style_path):\n","  # Load our images in \n","  content_image = load_and_process_img(content_path)\n","  style_image = load_and_process_img(style_path)\n","  \n","  # batch compute content and style features\n","  style_outputs = model(style_image)\n","  content_outputs = model(content_image)\n","  \n","  \n","  # Get the style and content feature representations from our model  \n","  style_features = [style_layer[0] for style_layer in style_outputs[:num_style_layers]]\n","  content_features = [content_layer[0] for content_layer in content_outputs[num_style_layers:]]\n","  return style_features, content_features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oVDhSo8iJunf","colab_type":"code","colab":{}},"source":["# compute the loss and gradients\n","def compute_loss(model, loss_weights, init_image, gram_style_features, content_features):\n","  style_weight, content_weight = loss_weights\n","  \n","  # Feed our init image through our model. returns the content and style representations at our desired layers\n","  model_outputs = model(init_image)\n","  \n","  style_output_features = model_outputs[:num_style_layers]\n","  content_output_features = model_outputs[num_style_layers:]\n","  \n","  style_score = 0\n","  content_score = 0\n","\n","  # Accumulate style losses from all layers. equally weighting each contribution of each loss layer\n","  weight_per_style_layer = 1.0 / float(num_style_layers)\n","  for target_style, comb_style in zip(gram_style_features, style_output_features):\n","    style_score += weight_per_style_layer * get_style_loss(comb_style[0], target_style)\n","    \n","  # Accumulate content losses from all layers \n","  weight_per_content_layer = 1.0 / float(num_content_layers)\n","  for target_content, comb_content in zip(content_features, content_output_features):\n","    content_score += weight_per_content_layer* get_content_loss(comb_content[0], target_content)\n","  \n","  style_score *= style_weight\n","  content_score *= content_weight\n","\n","  # Get total loss\n","  loss = style_score + content_score \n","  return loss, style_score, content_score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fwzYeOqOUH9_","colab_type":"code","colab":{}},"source":["def compute_grads(cfg):\n","  with tf.GradientTape() as tape: \n","    all_loss = compute_loss(**cfg)\n","  # Compute gradients wrt input image\n","  total_loss = all_loss[0]\n","  return tape.gradient(total_loss, cfg['init_image']), all_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pj_enNo6tACQ","colab_type":"code","colab":{}},"source":["import IPython.display\n","\n","def run_style_transfer(content_path, \n","                       style_path,\n","                       num_iterations=1000,\n","                       content_weight=1e4, \n","                       style_weight=1e-4): \n","  # not training so set false\n","  model = get_model() \n","  for layer in model.layers:\n","    layer.trainable = False\n","  \n","  # Get the style and content feature representations from specified intermediate layers\n","  style_features, content_features = get_feature_representations(model, content_path, style_path)\n","  gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n","  \n","  # Set initial image\n","  init_image = load_and_process_img(content_path)\n","  init_image = tf.Variable(init_image, dtype=tf.float32)\n","  # Create optimizer\n","  opt = tf.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=1e-1)\n","\n","  # For displaying intermediate images \n","  iter_count = 1\n","  \n","  # Store best result\n","  best_loss, best_img = float('inf'), None\n","  \n","  # Create a config \n","  loss_weights = (style_weight, content_weight)\n","  cfg = {\n","      'model': model,\n","      'loss_weights': loss_weights,\n","      'init_image': init_image,\n","      'gram_style_features': gram_style_features,\n","      'content_features': content_features\n","  }\n","    \n","  # For displaying\n","  num_rows = 2\n","  num_cols = 5\n","  display_interval = num_iterations/(num_rows*num_cols)\n","  start_time = time.time()\n","  global_start = time.time()\n","  \n","  norm_means = np.array([103.939, 116.779, 123.68])\n","  min_vals = -norm_means\n","  max_vals = 255 - norm_means   \n","  \n","  imgs = []\n","  for i in range(num_iterations):\n","    grads, all_loss = compute_grads(cfg)\n","    loss, style_score, content_score = all_loss\n","    opt.apply_gradients([(grads, init_image)])\n","    clipped = tf.clip_by_value(init_image, min_vals, max_vals)\n","    init_image.assign(clipped)\n","    end_time = time.time() \n","    \n","    if loss < best_loss:\n","      # Update best loss and best image from total loss. \n","      best_loss = loss\n","      best_img = deprocess_img(init_image.numpy())\n","\n","    if i % display_interval== 0:\n","      start_time = time.time()\n","      \n","      # Use the .numpy() method to get the solid numpy array\n","      plot_img = init_image.numpy()\n","      plot_img = deprocess_img(plot_img)\n","      imgs.append(plot_img)\n","      IPython.display.clear_output(wait=True)\n","      IPython.display.display_png(Image.fromarray(plot_img))\n","      print('Iteration: {}'.format(i))        \n","      print('Total loss: {:.4e}, ' \n","            'style loss: {:.4e}, '\n","            'content loss: {:.4e}, '\n","            'time: {:.4f}s'.format(loss, style_score, content_score, time.time() - start_time))\n","  print('Total time: {:.4f}s'.format(time.time() - global_start))\n","  IPython.display.clear_output(wait=True)\n","  plt.figure(figsize=(14,4))\n","  for i,img in enumerate(imgs):\n","      plt.subplot(num_rows,num_cols,i+1)\n","      plt.imshow(img)\n","      plt.xticks([])\n","      plt.yticks([])\n","      \n","  return best_img, best_loss "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lqTQN1PjulV9","colab_type":"code","colab":{}},"source":["# depreocess the output image to remove processing\n","def show_results(best_img, content_path, style_path, output_path, show_large_final=True):\n","  plt.figure(figsize=(10, 5))\n","  content = load_img(content_path) \n","  style = load_img(style_path)\n","\n","  plt.subplot(1, 2, 1)\n","  imshow(content, 'Content Image')\n","\n","  plt.subplot(1, 2, 2)\n","  imshow(style, 'Style Image')\n","\n","  if show_large_final: \n","    plt.figure(figsize=(40, 40))\n","\n","    plt.imshow(best_img)\n","    plt.imsave(output_path, best_img)\n","    plt.title('Output Image')\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LNX7Ck9024sz"},"source":["### Run Style Transfer"]},{"cell_type":"code","metadata":{"id":"Fes0FaaLtbqZ","colab_type":"code","colab":{}},"source":["# for x in range(1000):\n","    best, best_loss = run_style_transfer(content_path, style_path, num_iterations=100)\n","    Image.fromarray(best)\n","    show_results(best, content_path, style_path, output_path)"],"execution_count":0,"outputs":[]}]}